<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2018-4-5_note</title>
    <url>/2018/04/05/2018-4-5-note/</url>
    <content><![CDATA[<p><img src="/2018/04/05/2018-4-5-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="一天"><a href="#一天" class="headerlink" title="一天"></a>一天</h3><p>今天一直在做PPT了，避免明天要开组会，做了一份简单介绍目标检测的PPT。包含一点RCNN和YOLO的知识点。<br>所以就记录一下参考的内容</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="http://zh.gluon.ai/chapter_computer-vision/object-detection.html" target="_blank" rel="noopener">李沐讲解目标检测</a><br><a href="https://www.cs.cornell.edu/~dph/papers/seg-ijcv.pdf" target="_blank" rel="noopener">selective search采用的划分区域方法</a><br><a href="https://koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="noopener">selective search</a><br><a href="https://blog.csdn.net/dzJx2EOtaA24Adr/article/details/78129798" target="_blank" rel="noopener">基于深度学习的目标检测技术演进</a><br><a href="https://blog.csdn.net/dzJx2EOtaA24Adr/article/details/78129798" target="_blank" rel="noopener">一文搞懂目标检测</a></p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-4-4_note</title>
    <url>/2018/04/04/2018-4-4-note/</url>
    <content><![CDATA[<p><img src="/2018/04/04/2018-4-4-note/a.jpg" alt="图片"><br><a id="more"></a></p>
<h3 id="一天"><a href="#一天" class="headerlink" title="一天"></a>一天</h3><p>上了一天软硬件，这种课，说真的，浪费时间。</p>
<h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p>今天写毕设，发现两个神奇的东西</p>
<ul>
<li><a href="http://dict.cnki.net/dict_result.aspx" target="_blank" rel="noopener">cnki翻译助手</a></li>
<li><a href="http://www.fanyigou.net/language/turnZh.htm" target="_blank" rel="noopener">pdf等论文直接翻译</a></li>
</ul>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-4-3_note</title>
    <url>/2018/04/03/2018-4-3-note/</url>
    <content><![CDATA[<p><img src="/2018/04/03/2018-4-3-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="一天"><a href="#一天" class="headerlink" title="一天"></a>一天</h3><p>写点毕设，写不动了，写了500字。</p>
<h3 id="opencv简介"><a href="#opencv简介" class="headerlink" title="opencv简介"></a>opencv简介</h3><p>OpenCV是计算机视觉领域应用最广泛的开源工具包，基于C/C++，支持Linux/Windows/MacOS/Android/iOS，并提供了Python，Matlab和Java等语言的接口。<br>RGB图像在计算机中第一维度是高度，第二维度是宽度，第三个维度是通道数。其中在opencv中第三维度存在顺序是 <strong>BGR</strong></p>
<h3 id="opencv-基本函数"><a href="#opencv-基本函数" class="headerlink" title="opencv 基本函数"></a>opencv 基本函数</h3><h4 id="1-存取图像"><a href="#1-存取图像" class="headerlink" title="1. 存取图像"></a><strong>1. 存取图像</strong></h4><p>  读图像用cv2.imread()，可以按照不同模式读取，一般最常用到的是读取单通道灰度图，或者直接默认读取多通道。存图像用cv2.imwrite()，注意存的时候是没有单通道这一说的，根据保存文件名的后缀和当前的array维度，OpenCV自动判断存的通道，另外压缩格式还可以指定存储质量，代码例子：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取一张400x600分辨率的图像</span></span><br><span class="line">color_img = cv2.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">print(color_img.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接读取单通道</span></span><br><span class="line">gray_img = cv2.imread(<span class="string">'lenna.jpg'</span>, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">print(gray_img.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把单通道图片保存后，再读取，仍然是3通道，相当于把单通道值复制到3个通道保存</span></span><br><span class="line">cv2.imwrite(<span class="string">'test_grayscale.jpg'</span>, gray_img)</span><br><span class="line">reload_grayscale = cv2.imread(<span class="string">'test_grayscale.jpg'</span>)</span><br><span class="line">print(reload_grayscale.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cv2.IMWRITE_JPEG_QUALITY指定jpg质量，范围0到100，默认95，越高画质越好，文件越大</span></span><br><span class="line">cv2.imwrite(<span class="string">'test_imwrite.jpg'</span>, color_img, (cv2.IMWRITE_JPEG_QUALITY, <span class="number">80</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># cv2.IMWRITE_PNG_COMPRESSION指定png质量，范围0到9，默认3，越高文件越小，画质越差</span></span><br><span class="line">cv2.imwrite(<span class="string">'test_imwrite.png'</span>, color_img, (cv2.IMWRITE_PNG_COMPRESSION, <span class="number">5</span>))</span><br></pre></td></tr></table></figure></p>
<h4 id="2-OpenCV窗口"><a href="#2-OpenCV窗口" class="headerlink" title="2. OpenCV窗口"></a><strong>2. OpenCV窗口</strong></h4><p>OpenCV显示一幅图片的函数是cv2.imshow()，第一个参数是显示图片的窗口名称，第二个参数是图片的array。不过如果直接执行这个函数的话，什么都不会发生，因为这个函数得配合cv2.waitKey()一起使用。cv2.waitKey()指定当前的窗口显示要持续的毫秒数，比如cv2.waitKey(1000)就是显示一秒，然后窗口就关闭了。比较特殊的是cv2.waitKey(0)，并不是显示0毫秒的意思，而是一直显示，直到有键盘上的按键被按下，或者鼠标点击了窗口的小叉子才关闭。cv2.waitKey()的默认参数就是0，所以对于图像展示的场景，cv2.waitKey()或者cv2.waitKey(0)是最常用的：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">cv2.imshow(<span class="string">'Lenna'</span>, img)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure></p>
<h4 id="3-视频功能"><a href="#3-视频功能" class="headerlink" title="3. 视频功能"></a><strong>3. 视频功能</strong></h4><p>视频中最常用的就是从视频设备采集图片或者视频，或者读取视频文件并从中采样。所以比较重要的也是两个模块，一个是VideoCapture，用于获取相机设备并捕获图像和视频，或是从文件中捕获。还有一个VideoWriter，用于生成视频。一个延时摄影视频的小例子：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">interval = <span class="number">60</span>       	<span class="comment"># 捕获图像的间隔，单位：秒</span></span><br><span class="line">num_frames = <span class="number">500</span>    	<span class="comment"># 捕获图像的总帧数</span></span><br><span class="line">out_fps = <span class="number">24</span>        	<span class="comment"># 输出文件的帧率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># VideoCapture(0)表示打开默认的相机</span></span><br><span class="line">cap = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取捕获的分辨率</span></span><br><span class="line">size =(int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),</span><br><span class="line">       int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))</span><br><span class="line">       </span><br><span class="line"><span class="comment"># 设置要保存视频的编码，分辨率和帧率</span></span><br><span class="line">video = cv2.VideoWriter(</span><br><span class="line">    <span class="string">"time_lapse.avi"</span>, </span><br><span class="line">    cv2.VideoWriter_fourcc(<span class="string">'M'</span>,<span class="string">'P'</span>,<span class="string">'4'</span>,<span class="string">'2'</span>), </span><br><span class="line">    out_fps, </span><br><span class="line">    size</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于一些低画质的摄像头，前面的帧可能不稳定，略过</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">42</span>):</span><br><span class="line">    cap.read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始捕获，通过read()函数获取捕获的帧</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_frames):</span><br><span class="line">        _, frame = cap.read()</span><br><span class="line">        video.write(frame)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果希望把每一帧也存成文件，比如制作GIF，则取消下面的注释</span></span><br><span class="line">        <span class="comment"># filename = '&#123;:0&gt;6d&#125;.png'.format(i)</span></span><br><span class="line">        <span class="comment"># cv2.imwrite(filename, frame)</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Frame &#123;&#125; is captured.'</span>.format(i))</span><br><span class="line">        time.sleep(interval)</span><br><span class="line"><span class="keyword">except</span> KeyboardInterrupt:</span><br><span class="line">    <span class="comment"># 提前停止捕获</span></span><br><span class="line">    print(<span class="string">'Stopped! &#123;&#125;/&#123;&#125; frames captured!'</span>.format(i, num_frames))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 释放资源并写入视频文件</span></span><br><span class="line">video.release()</span><br><span class="line">cap.release()</span><br></pre></td></tr></table></figure></p>
<h3 id="Haar分类器介绍"><a href="#Haar分类器介绍" class="headerlink" title="Haar分类器介绍"></a>Haar分类器介绍</h3><p>Haar分类器包含以下部分：</p>
<ol>
<li>使用Haar-like特征做检测。</li>
<li>使用积分图（Integral Image）对Haar-like特征求值进行加速。</li>
<li>使用AdaBoost算法训练区分人脸和非人脸的强分类器。</li>
<li>使用筛选式级联把强分类器级联到一起，提高准确率。<h4 id="Haar-like"><a href="#Haar-like" class="headerlink" title="Haar-like"></a>Haar-like</h4>通俗的来讲，Haar-like就是作为人脸特征。Haar特征值反映了图像的灰度变化情况。例如：脸部的一些特征能由矩形特征简单的描述，如：眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。假设在人脸检测时我们需要有这么一个子窗口在待检测的图片窗口中不断的移位滑动，子窗口每到一个位置，就会计算出该区域的特征，然后用我们训练好的级联分类器对该特征进行筛选，一旦该特征通过了所有强分类器的筛选，则判定该区域为人脸。</li>
</ol>
<p>![其中一种特征]!(./images/1522743350520.jpg)<br>将上面的任意一个矩形放到人脸区域上，然后，将白色区域的像素和减去黑色区域的像素和，得到的值我们暂且称之为人脸特征值，如果你把这个矩形放到一个非人脸区域，那么计算出的特征值应该和人脸特征值是不一样的，而且越不一样越好，所以这些方块的目的就是把人脸特征量化，以区分人脸和非人脸。</p>
<h4 id="分类器其他部分原理参考-1"><a href="#分类器其他部分原理参考-1" class="headerlink" title="分类器其他部分原理参考[1]."></a>分类器其他部分原理参考[1].</h4><p><code># 主要我也不清楚</code></p>
<h3 id="使用api检测人脸"><a href="#使用api检测人脸" class="headerlink" title="使用api检测人脸"></a>使用api检测人脸</h3><ul>
<li><p>读取图片</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.imread(imgpath)</span><br></pre></td></tr></table></figure>
</li>
<li><p>灰度转换<br>将读取的图像转换成 cv2.COLOR_BGR2GRAY格式   ,具体的算法可见<a href="http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html#cvtcolor" target="_blank" rel="noopener">cvtColor</a><br>这里的检测的依据是 哈尔特征 ,转换后每个点的RGB数据变成了一维的灰度,计算的强度减小,其实不转换也是可以的.(本文图像测试可行)</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取人脸识别训练数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">face_cascade = cv2.CascadeClassifier(haarpath)</span><br></pre></td></tr></table></figure>
<p>这是初始化opencv的Cascade Classification,详情可参见<a href="https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html" target="_blank" rel="noopener">Cascade Classification</a> , 它的作用就是产生一个检测器,检测的依据全都储存在参数所代表的那个xml文件中,这个xml文件可以在 <a href="https://github.com/Itseez/opencv/tree/master/data/haarcascades" target="_blank" rel="noopener">opencv-github</a> 中获得,官方提供的还有眼睛,树等其他对象的识别数据,这些数据就是一张一张图像训练出来的, <a href="http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html" target="_blank" rel="noopener">这里</a> 有一篇讲解如何自己训练得到检测数据的.</p>
</li>
<li><p>探测人脸<br>对人脸的识别过程</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">faces = faceCascade.detectMultiScale(</span><br><span class="line">	gray,</span><br><span class="line">	scaleFactor=<span class="number">1.15</span>,</span><br><span class="line">	minNeighbors=<span class="number">5</span>,</span><br><span class="line">	minSize=(<span class="number">5</span>,<span class="number">5</span>),</span><br><span class="line">	flags = cv2.cv.CV_HAAR_SCALE_IMAGE</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这是这段代码中的核心,这几个参数最终决定能否检测出.方法<a href="http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html?highlight=detectmultiscale#cascadeclassifier-detectmultiscale" target="_blank" rel="noopener">detectMultiScale</a><br>参数作用.<br>gray               :    进行检测的图像,这里是转换后的,<br>scaleFactor     :    官网文档说是每次图片缩小的比例,其实可以这么理解,距离相机不同的距离,物体大小是不一样的,在物体大小不一致的情况下识别一个东西是不方便的,这就需要进行多次的缩放,这就是这个参数的作用.<br>minNeighbors :   可以理解为每次检测时,对检测点(Scale)周边多少有效点同时检测,因为可能选取的检测点大小不足而导致遗漏<br>minSize          :   检测点的最小值,或者说就是检测点的最终值<br>flags              :   这个参数在新版的opencv是不需要的,暂时不管了,注释掉同样可以<br>返回值    ： 是个ndarray是一个二维数组,行数是检测出的对象的个数,每行是检测到的矩形区域的坐标(左上,右下)</p>
</li>
<li><p>画框。下面的这个函数最后一个参数指定的就是画笔的大小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv2.rectangle(image,(x,y),(x+w,y+w),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://my.oschina.net/chinesezhx/blog/520917" target="_blank" rel="noopener">python_opencv人脸检测</a></p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-4-1_note</title>
    <url>/2018/04/01/2018-4-1-note/</url>
    <content><![CDATA[<p><img src="/2018/04/01/2018-4-1-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="一天"><a href="#一天" class="headerlink" title="一天"></a>一天</h3><p>空白的一天，之前的笔记也忘了整理了。</p>
<h3 id="opencv3-2-安装"><a href="#opencv3-2-安装" class="headerlink" title="opencv3.2 安装"></a>opencv3.2 安装</h3><ul>
<li>查看opencv 版本<code>pkg-config    --modversion    opencv</code></li>
<li><p>安装依赖</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo	apt-get	install	build-essential	libgtk2.0-dev	libjpeg-dev	libtiff5-dev	libjasper-dev	libopenexr-dev	cmake	libeigen3-dev	yasm	libfaac-dev	libtheora-dev	libx264-dev	libv4l-dev	libavcodec-dev	libavformat-dev	libswscale-dev	libv4l-dev	ffmpeg</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载opencv ，解压到某个目录下,进入目录</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd	opencv-***</span><br></pre></td></tr></table></figure>
</li>
<li><p>新建编译目录</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mkdir	release </span><br><span class="line">cd release</span><br></pre></td></tr></table></figure>
</li>
<li><p>编译</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> cmake	-D	MAKE_BUILD_TYPE=RELEASE	-D	         CMAKE_INSTALL_PREFIX=/usr/local	-D	               WITH_OPENCL=OFF	-D	WITH_CUD</span><br><span class="line"> A=OFF	..</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo	make	install</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置链接库</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo	/bin/bash	-c	&apos;echo	&quot;/usr/local/lib&quot;	&gt;	/etc/ld.so.conf.d/opencv.conf&apos;</span><br><span class="line">sudo	ldconfig</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证，打开opencvtest文件夹</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> make</span><br><span class="line">./DisplayImage	&lt;picture	path	here&gt;</span><br><span class="line"># 将 &lt;picture	path	here&gt; 替换为任意一张图片的路径</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="安装OpenCL"><a href="#安装OpenCL" class="headerlink" title="安装OpenCL"></a>安装OpenCL</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo	apt-get	install	clinfo</span><br></pre></td></tr></table></figure>
<ul>
<li>安装cuda（之前安装好了）</li>
<li>安装与OpenCL有关的包 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo	apt-get	install	nvidia-modprobe	nvidia-libopencl1-&lt;version	here&gt;	nvidia-opencl-icd-&lt;version	here&gt; nvidia-opencl-dev</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h3><p><a href="https://blog.csdn.net/li_wen01/article/details/71641408" target="_blank" rel="noopener">ippicv_linux_20151201失败</a></p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-3-31_note</title>
    <url>/2018/03/31/2018-3-31-note/</url>
    <content><![CDATA[<p><img src="/2018/03/31/2018-3-31-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="一天"><a href="#一天" class="headerlink" title="一天"></a>一天</h3><p>上午写了一点毕设，下午啥也没干，晚上b站发现新大陆，看了一晚上李沐大神的视频关于<a href="https://space.bilibili.com/209599371?spm_id_from=333.338.v_upinfo.3#/video" target="_blank" rel="noopener">目标检测的部分</a>,等到看完完整的再总结一下吧。</p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-3-30_note</title>
    <url>/2018/03/30/2018-3-30-note/</url>
    <content><![CDATA[<p><img src="/2018/03/30/2018-3-30-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="毕设"><a href="#毕设" class="headerlink" title="毕设"></a>毕设</h3><p>装texlive 好烦，装了好几次每次都安装一个多小时，然后失败。最后直接下载整个包，安装。明天争取写完 instruction和related work。</p>
<h3 id="cnn"><a href="#cnn" class="headerlink" title="cnn"></a>cnn</h3><p>下午看了一个cnn的推导过程，以及cnn怎样反向传播，看了两遍，推导公式推导一半就放弃。等到毕设搞完再推导。<a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="noopener">cnn全面讲解</a></p>
<h3 id="Andrej-Karpathy"><a href="#Andrej-Karpathy" class="headerlink" title="Andrej Karpathy"></a>Andrej Karpathy</h3><p>看了一篇李飞飞高徒 Andrej Karpathy写的<a href="http://karpathy.github.io/2016/09/07/phd/" target="_blank" rel="noopener">博士生存指南</a>,看的是<a href="https://baijia.baidu.com/s?old_id=618176" target="_blank" rel="noopener">翻译版</a>.讲了一些写论文技巧和禁忌，还有一些人生。</p>
<h3 id="李飞飞机器视觉公开课"><a href="#李飞飞机器视觉公开课" class="headerlink" title="李飞飞机器视觉公开课"></a>李飞飞机器视觉公开课</h3><p>前两节水课，第三节讲了knn用于cifar.</p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-3-28_note</title>
    <url>/2018/03/28/2018-3-28-note/</url>
    <content><![CDATA[<p><img src="/2018/03/28/2018-3-28-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="差不多搞明白了yolov1的细节"><a href="#差不多搞明白了yolov1的细节" class="headerlink" title="差不多搞明白了yolov1的细节"></a>差不多搞明白了yolov1的细节</h3><p><a href="https://blog.csdn.net/hrsstudy/article/details/70305791" target="_blank" rel="noopener">yolov1笔记</a></p>
<p><img src="http://ompb5dzt6.bkt.clouddn.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1522249588603.jpg" alt=" yolov1"></p>
<p><img src="http://ompb5dzt6.bkt.clouddn.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1522249630080.jpg" alt="yolov1"></p>
<h3 id="yolov2-code"><a href="#yolov2-code" class="headerlink" title="yolov2 code"></a>yolov2 code</h3><p><a href="https://github.com/JaviLaplaza/Pytorch-YOLOv2" target="_blank" rel="noopener">Pytorch-YOLOv2</a><br><a href="https://github.com/ayooshkathuria/PyTorch-YOLO-v2" target="_blank" rel="noopener">好像有点复杂的一个</a><br><a href="https://github.com/luliyucoordinate/YOLOv2-pytorch" target="_blank" rel="noopener">有video?</a><br><a href="https://github.com/xiongzihua/pytorch-YOLO-v1" target="_blank" rel="noopener">带可视化的v1</a><br><a href="https://github.com/AceCoooool/YOLO-pytorch" target="_blank" rel="noopener">可以用相机</a><br><a href="https://github.com/txdat/tf-yolo-v2" target="_blank" rel="noopener">有点水的一个？</a><br><a href="https://github.com/ruiminshen/yolo2-pytorch" target="_blank" rel="noopener">还是带video</a></p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-3-27_note</title>
    <url>/2018/03/27/2018-3-27-note/</url>
    <content><![CDATA[<p><img src="/2018/03/27/2018-3-27-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="sublime-Latex"><a href="#sublime-Latex" class="headerlink" title="sublime + Latex"></a>sublime + Latex</h3><p><a href="https://blog.csdn.net/jenyzhang/article/details/78271711" target="_blank" rel="noopener">参考</a><br><a href="https://www.codecogs.com/latex/eqneditor.php?lang=zh-cn" target="_blank" rel="noopener">latex在线公式编辑</a><br><a href="https://github.com/krlmlr/Excel2LaTeX" target="_blank" rel="noopener">latex excel2latex</a><br><a href="http://www.tablesgenerator.com/" target="_blank" rel="noopener">在线生成latex表格</a></p>
<h3 id="配置遇到问题"><a href="#配置遇到问题" class="headerlink" title="配置遇到问题"></a>配置遇到问题</h3><p><img src="/2018/03/27/2018-3-27-note/1522155572349.jpg" alt="ctrl+b"></p>
<h3 id="YOLOv1基础版本"><a href="#YOLOv1基础版本" class="headerlink" title="YOLOv1基础版本"></a>YOLOv1基础版本</h3><p>继续昨天的部分<a href="https://github.com/makora9143/yolo-pytorch/tree/master/yolov1" target="_blank" rel="noopener">v1</a>.</p>
<ul>
<li>model.py 部分有一个DarkNet类和YOLO类。当没有模型传入时，直接前部分网络使用Darknet,yolo又再后面加了几层。有训练好模型传入时，yolo在训练好的模型后面加入几层。（明天对比论文看一下是否每一层都是对应论文实现的）</li>
<li>train.py 使用是通过没次迭代选取最好的一个模型保存</li>
<li>loss.py 计算出了类别概率，置信度，以及坐标，然后计算使用平方误差计算loss.<h3 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a>YOLOv2</h3></li>
</ul>
<p><a href="https://github.com/PseudoSuffix/PyTorch-YOLO" target="_blank" rel="noopener">yolov2基础版</a><br><a href="https://github.com/A-Jacobson/YoloV2" target="_blank" rel="noopener">yolov2 使用dataloader的数据读取方式</a></p>
<h3 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h3><p>明天对比论文每一具体细节看一下v1的参数和置信度等计算方法。然后找一些v2简单版本的实现。</p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-3-26_note</title>
    <url>/2018/03/26/2018-3-26-note/</url>
    <content><![CDATA[<p><img src="/2018/03/26/2018-3-26-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="yolov1-简单版"><a href="#yolov1-简单版" class="headerlink" title="yolov1 简单版"></a>yolov1 简单版</h3><p><a href="https://github.com/Ja1r0/yolo-pytorch/blob/master/network.py" target="_blank" rel="noopener">该版本</a>只是实现了其中卷积层和全连接网络结构结构，并不是一个能真正运行的网络。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,S,B,C)</span>:</span></span><br><span class="line">    super(yolo,self).__init__()</span><br><span class="line">    self.output_dim=S*S*(B*<span class="number">5</span>+C)</span><br><span class="line">    self.layers1=BasicConv2d([(<span class="number">3</span>,<span class="number">64</span>,<span class="number">7</span>,<span class="number">2</span>,<span class="number">3</span>)],<span class="keyword">True</span>)</span><br><span class="line">    self.layers2=BasicConv2d([(<span class="number">64</span>,<span class="number">192</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)],<span class="keyword">True</span>)</span><br><span class="line">    self.layers3=BasicConv2d([(<span class="number">192</span>,<span class="number">128</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>),(<span class="number">128</span>,<span class="number">256</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                             (<span class="number">256</span>,<span class="number">256</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>),(<span class="number">256</span>,<span class="number">512</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)],<span class="keyword">True</span>)</span><br><span class="line">    self.layers4=BasicConv2d([(<span class="number">512</span>,<span class="number">256</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>),(<span class="number">256</span>,<span class="number">512</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                             (<span class="number">512</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>,<span class="number">0</span>), (<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                             (<span class="number">512</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>,<span class="number">0</span>), (<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                             (<span class="number">512</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>,<span class="number">0</span>), (<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                             (<span class="number">512</span>,<span class="number">512</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>),(<span class="number">512</span>,<span class="number">1024</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">                             ],<span class="keyword">True</span>)</span><br><span class="line">    self.layers5=BasicConv2d([(<span class="number">1024</span>,<span class="number">512</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>),(<span class="number">512</span>,<span class="number">1024</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                             (<span class="number">1024</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>,<span class="number">0</span>), (<span class="number">512</span>, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                             (<span class="number">1024</span>,<span class="number">1024</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1024</span>,<span class="number">1024</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">                             ],<span class="keyword">False</span>)</span><br><span class="line">    self.layers6=BasicConv2d([(<span class="number">1024</span>,<span class="number">1024</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1024</span>,<span class="number">1024</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)],<span class="keyword">False</span>)</span><br><span class="line">    self.classifier=nn.Sequential(nn.Linear(<span class="number">7</span>*<span class="number">7</span>*<span class="number">1024</span>,<span class="number">4096</span>),</span><br><span class="line">                                  nn.Linear(<span class="number">4096</span>,self.output_dim))</span><br></pre></td></tr></table></figure></p>
<p>这一部分只是定义了一些网络参数，然后进行前向传播，在测试时反向传播一下。对于yolo中的细节并未进行详细实现。</p>
<h3 id="yolov1-初级版"><a href="#yolov1-初级版" class="headerlink" title="yolov1 初级版"></a>yolov1 初级版</h3><p>代码部分由以下几个文件组成：<br><img src="/2018/03/26/2018-3-26-note/1522080029269.jpg" alt="代码结构"><br>数据部分由两部分，图片和标注，标注使用json格式<br><img src="/2018/03/26/2018-3-26-note/1522080111963.jpg" alt="enter description here"></p>
<h3 id="data-py阅读"><a href="#data-py阅读" class="headerlink" title="data.py阅读"></a>data.py阅读</h3><p>今天就看了数据读取和处理。<br>数据格式如下，包括label,标注框的左上角x_y以及w_h。还包括图片形状和路径。<br><img src="/2018/03/26/2018-3-26-note/1522080219471.jpg" alt="enter description here"><br><img src="/2018/03/26/2018-3-26-note/1522080256138.jpg" alt="enter description here"><br>数据处理部分包括以下几个函数</p>
<ul>
<li>train_batches(batch_size=1, use_cuda=False)<br>随机选取batch_size大小的下标<br>返回这些下表对应的数据(get_datas)</li>
<li>get_datas(idx, use_cuda=False)<br>输入一组下表值，返回图像和标签(这个明天还要再看一下。。。)</li>
<li>load_data(i)<br>输入一个下表，返回该图片，以及[w,h,[标注信息],[。。。]]</li>
<li><p>create_label(chunk)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input: chunk</span><br><span class="line">  chunk is list object.</span><br><span class="line">      -&gt; [img_path, [w, h, [[label, xn, yn, xx, yx], [label, xn, yn, xx, yx], ..., [label, xn, yn, xx, yx]]]</span><br><span class="line">          img_path: &quot;path_to_img/sample.jpg&quot;,</span><br><span class="line">          w: image width,</span><br><span class="line">          h: image height,</span><br><span class="line">          label: object class,</span><br><span class="line">          xn, yn: topleft coordinates,</span><br><span class="line">          xx, yx: bottomright coordinates</span><br></pre></td></tr></table></figure>
<p>  返回的是图像，以及置信度信息，坐标信息等。</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y_true = &#123;</span><br><span class="line">       &apos;class_probs&apos;: class_probs,</span><br><span class="line">       &apos;confs&apos;: confs,</span><br><span class="line">       &apos;coord&apos;: coord,</span><br><span class="line">       &apos;proid&apos;: proid,</span><br><span class="line">       &apos;areas&apos;: areas,</span><br><span class="line">       &apos;upleft&apos;: upleft,</span><br><span class="line">       &apos;bottomright&apos;: bottomright</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="python-坑"><a href="#python-坑" class="headerlink" title="python 坑"></a>python 坑</h3><p><img src="/2018/03/26/2018-3-26-note/1522079551660.jpg" alt="python坑"></p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-3-25_note</title>
    <url>/2018/03/25/2018-3-25-note/</url>
    <content><![CDATA[<p><img src="/2018/03/25/2018-3-25-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="python-广播"><a href="#python-广播" class="headerlink" title="python 广播"></a>python 广播</h3><p>广播的原则：如果两个数组的后缘维度（trailing dimension从末尾开始算起的维度）的轴 长度相符或者其中一方为1，那么认为是广播兼容的。广播会在缺失和（或）长度为1的维度上进行。</p>
<p><img src="/2018/03/25/2018-3-25-note/1521975299841.jpg" alt="二维数组在轴1上广播"></p>
<p><img src="/2018/03/25/2018-3-25-note/1521975335201.jpg" alt="三维数组在轴0上广播"></p>
<p>根据广播的原.则，较小数组的广播维必须为1。于是就有了一个很普遍的问题，即为了广播专门增加一个新的长度为1的维度。</p>
<h3 id="tensorflow的bool-mask-tensor-mask-axis-0"><a href="#tensorflow的bool-mask-tensor-mask-axis-0" class="headerlink" title="tensorflow的bool_mask(tensor,mask,axis=0)"></a>tensorflow的bool_mask(tensor,mask,axis=0)</h3><p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py" target="_blank" rel="noopener">tensorflow源码</a><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Args:</span><br><span class="line">  tensor:  N-D tensor.</span><br><span class="line">  mask:  K-D boolean tensor, K &lt;= N <span class="keyword">and</span> K must be known statically.</span><br><span class="line">  name:  A name <span class="keyword">for</span> this operation (optional).</span><br><span class="line">  axis:  A <span class="number">0</span>-D int Tensor representing the axis <span class="keyword">in</span> `tensor` to mask <span class="keyword">from</span>.</span><br><span class="line">    By default, axis <span class="keyword">is</span> <span class="number">0</span> which will mask <span class="keyword">from</span> the first dimension. Otherwise</span><br><span class="line">    K + axis &lt;= N.</span><br><span class="line">Returns:</span><br><span class="line">  (N-K+<span class="number">1</span>)-dimensional tensor populated by entries <span class="keyword">in</span> `tensor` corresponding</span><br><span class="line">  to `<span class="keyword">True</span>` values <span class="keyword">in</span> `mask`.</span><br></pre></td></tr></table></figure></p>
<p>pytorch中使用selected_mask()得到的是一个一维tensor,所以view一下就可以得到类似的结果（tensor二维时验证没问题）。</p>
<h3 id="nms计算"><a href="#nms计算" class="headerlink" title="nms计算"></a>nms计算</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8  </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">py_cpu_nms</span><span class="params">(dets, thresh)</span>:</span>  </span><br><span class="line">    <span class="string">"""Pure Python NMS baseline."""</span>  </span><br><span class="line">    x1 = dets[:, <span class="number">0</span>]  </span><br><span class="line">    y1 = dets[:, <span class="number">1</span>]  </span><br><span class="line">    x2 = dets[:, <span class="number">2</span>]  </span><br><span class="line">    y2 = dets[:, <span class="number">3</span>]  </span><br><span class="line">    scores = dets[:, <span class="number">4</span>]  <span class="comment">#bbox打分</span></span><br><span class="line">  </span><br><span class="line">    areas = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)  </span><br><span class="line"><span class="comment">#打分从大到小排列，取index  </span></span><br><span class="line">    order = scores.argsort()[::<span class="number">-1</span>]  </span><br><span class="line"><span class="comment">#keep为最后保留的边框  </span></span><br><span class="line">    keep = []  </span><br><span class="line">    <span class="keyword">while</span> order.size &gt; <span class="number">0</span>:  </span><br><span class="line"><span class="comment">#order[0]是当前分数最大的窗口，肯定保留  </span></span><br><span class="line">        i = order[<span class="number">0</span>]  </span><br><span class="line">        keep.append(i)  </span><br><span class="line"><span class="comment">#计算窗口i与其他所有窗口的交叠部分的面积</span></span><br><span class="line">        xx1 = np.maximum(x1[i], x1[order[<span class="number">1</span>:]])  </span><br><span class="line">        yy1 = np.maximum(y1[i], y1[order[<span class="number">1</span>:]])  </span><br><span class="line">        xx2 = np.minimum(x2[i], x2[order[<span class="number">1</span>:]])  </span><br><span class="line">        yy2 = np.minimum(y2[i], y2[order[<span class="number">1</span>:]])  </span><br><span class="line">  </span><br><span class="line">        w = np.maximum(<span class="number">0.0</span>, xx2 - xx1 + <span class="number">1</span>)  </span><br><span class="line">        h = np.maximum(<span class="number">0.0</span>, yy2 - yy1 + <span class="number">1</span>)  </span><br><span class="line">        inter = w * h  </span><br><span class="line"><span class="comment">#交/并得到iou值  </span></span><br><span class="line">        ovr = inter / (areas[i] + areas[order[<span class="number">1</span>:]] - inter)  </span><br><span class="line"><span class="comment">#inds为所有与窗口i的iou值小于threshold值的窗口的index，其他窗口此次都被窗口i吸收  </span></span><br><span class="line">        inds = np.where(ovr &lt;= thresh)[<span class="number">0</span>]  </span><br><span class="line"><span class="comment">#order里面只保留与窗口i交叠面积小于threshold的那些窗口，由于ovr长度比order长度少1(不包含i)，所以inds+1对应到保留的窗口</span></span><br><span class="line">        order = order[inds + <span class="number">1</span>]  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> keep</span><br></pre></td></tr></table></figure>
<p>实现逻辑</p>
<ol>
<li>按打分最高到最低将BBox排序 ，例如：A B C D E F</li>
<li>A的分数最高，保留。从B-E与A分别求重叠率IoU，假设B、D与A的IoU小于阈值，那么B和D可以认为是重复标记去除</li>
<li>余下C E F，重复前面两步。</li>
</ol>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>这份网易课堂的搞不定了。弃坑。在github上有两份<a href="https://github.com/Ja1r0/yolo-pytorch" target="_blank" rel="noopener">yolov1最简单</a> <a href="https://github.com/xiongzihua/pytorch-YOLO-v1" target="_blank" rel="noopener">yolov1有可视化</a> <a href="https://github.com/makora9143/yolo-pytorch" target="_blank" rel="noopener">yolov1模型适中</a>的代码不长，效果一般，但是先搞一遍再说。</p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-3-24_note</title>
    <url>/2018/03/24/2018-3-24-note/</url>
    <content><![CDATA[<p><img src="/2018/03/24/2018-3-24-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<hr>
<h3 id="被pytorch和tensorflow毒害的一天"><a href="#被pytorch和tensorflow毒害的一天" class="headerlink" title="被pytorch和tensorflow毒害的一天"></a>被pytorch和tensorflow毒害的一天</h3><p>首先是一个关于python广播的问题。<br>a = torch.randn(3,1).abs_()<br>b = torch.ByteTensor([1,0,1]) #b形状为（3，）<br>b = torch.unsqueeze(b,-1) #b形状为（3，1）<br>a = torch.randn(3,4)<br>c = torch.masked_select(a,b) # 此时可以在列上广播<br># 当b的形状为（3，）不能在列上广播。<br># 关于python广播机制，明天做个总结<br>tensorflow中的argmax()和max()函数，在pytorch中可以用max()代替</p>
<h3 id="yolo-filter-boxes"><a href="#yolo-filter-boxes" class="headerlink" title="yolo filter_boxes"></a>yolo filter_boxes</h3><p>对于昨天计算box_score做验证<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_filter_boxes</span><span class="params">(box_confidence,boxes,box_class_probs,threshold=<span class="number">0</span>)</span>:</span></span><br><span class="line">    box_score = box_confidence * box_class_probs</span><br><span class="line">    box_class_scores,box_classes = torch.max(box_score,dim=<span class="number">-1</span>)</span><br><span class="line">    filtering_mask = box_class_scores &gt;= threshold</span><br><span class="line">    filtering_mask = torch.unsqueeze(filtering_mask,dim=<span class="number">-1</span>)</span><br><span class="line"><span class="comment">#     print(filtering_mask)</span></span><br><span class="line">    scores = torch.masked_select(box_class_scores,filtering_mask)</span><br><span class="line">    boxes = torch.masked_select(boxes, filtering_mask)</span><br><span class="line">    classes = torch.masked_select(box_classes, filtering_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> scores, boxes,classes</span><br><span class="line"></span><br><span class="line">box_confidence = torch.randn([<span class="number">19</span>,<span class="number">19</span>,<span class="number">5</span>,<span class="number">1</span>])</span><br><span class="line">boxes = torch.randn([<span class="number">19</span>,<span class="number">19</span>,<span class="number">5</span>,<span class="number">4</span>])</span><br><span class="line">box_class_probs = torch.randn([<span class="number">19</span>,<span class="number">19</span>,<span class="number">5</span>,<span class="number">80</span>])</span><br><span class="line"></span><br><span class="line">scores,boxes,classes = yolo_filter_boxes(box_confidence,boxes,box_class_probs,threshold=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure></p>
<p>比较难受的是tf.boolean_mask()，和pytorch的masked_select（）的广播机制，明天在好好看看。目前通过将pytorch将mask增加一个维度解决不能广播的问题</p>
<h3 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h3><p>pytorch 没有reshape,用view代替reshape。<br>pytorch unsqueeze可以增加一个维度，squeeze可以压缩维度。<br>pytorch masked_select()可以通过bool矩阵选取元素。<br>pytorch max() 返回值第一个是按某个维度的最大值，第二个是对应下标。</p>
<h3 id="感慨"><a href="#感慨" class="headerlink" title="感慨"></a>感慨</h3><p>翻着文档从pytorch中找到tensorflow的替代函数好难啊。不知道有啥简洁点的方式。</p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-3-23_note</title>
    <url>/2018/03/23/2018-3-23-note/</url>
    <content><![CDATA[<p><img src="/2018/03/23/2018-3-23-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="anchor-box"><a href="#anchor-box" class="headerlink" title="anchor box"></a>anchor box</h3><p>看不懂，明天再说。实在不理解，也没找到讲得好的博客。</p>
<h3 id="吴恩达cnn课程编程作业"><a href="#吴恩达cnn课程编程作业" class="headerlink" title="吴恩达cnn课程编程作业"></a>吴恩达cnn课程编程作业</h3><h4 id="关于模型"><a href="#关于模型" class="headerlink" title="关于模型"></a>关于模型</h4><p>识别类别有80类</p>
<ul>
<li>输入时一个batch的图片，所以输入形状为（m,608,608,3）m为batch_size.</li>
<li>对于每个bbox(bounding box,这里为19*19)输出为一个85维向量（$p_c,b_x,b_y,b_h,b_w,c$,）其中c是80维向量指的是类别，$p_c$表示该box是否存在object。</li>
<li>然后对于每个bbox有5个anchor boses.相当于对每个bbox,预测5个boxes.所以结构流程为IMAGE(m,608,608,3)-&gt;DEEP CNN-&gt;ENCODING(m,19,19,5,85).<h4 id="计算score"><a href="#计算score" class="headerlink" title="计算score"></a>计算score</h4>score = pc*c ：逐点相乘</li>
</ul>
<p><img src="/2018/03/23/2018-3-23-note/1521822466208.jpg" alt="score计算方式"></p>
<h4 id="使用threshold-在score上进行过滤"><a href="#使用threshold-在score上进行过滤" class="headerlink" title="使用threshold 在score上进行过滤"></a>使用threshold 在score上进行过滤</h4><p>几个参数意义：</p>
<ul>
<li>box_confidence: box的置信度，表示box是否含有object，形状为（19*19,5,1）.</li>
<li>boxes: boxes的信息( $b_x,b_y,b_h,b_w$ ).形状为（19*19,5,4）</li>
<li>box_class_probs: 5个anchor在80个类别上的预测概率，形状为（19*19,5,80）.</li>
</ul>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018_3_22_note</title>
    <url>/2018/03/22/2018-3-22-note/</url>
    <content><![CDATA[<p><img src="/2018/03/22/2018-3-22-note/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="coursera-cnn编程作业"><a href="#coursera-cnn编程作业" class="headerlink" title="coursera cnn编程作业"></a>coursera cnn编程作业</h3><p>不能使用coursera，所以在csdn上找到了<a href="https://blog.csdn.net/koala_tree/article/details/78690396" target="_blank" rel="noopener">作业</a>。作业目的是搭建一个yolo网络，使用预训练网络测试一些图片。今天大概看了一下流程，明天开始写代码。因为coursera上使用的是keras,但是准备使用pytorch实现。</p>
<h3 id="一个有趣的小项目"><a href="#一个有趣的小项目" class="headerlink" title="一个有趣的小项目"></a>一个有趣的小项目</h3><p>从零开始码一个皮卡丘检测器。网络使用的是ssd，讲解的内容很详细。有空看。</p>
<h3 id="有趣的发现"><a href="#有趣的发现" class="headerlink" title="有趣的发现"></a>有趣的发现</h3><p>今天重新使用yolo跑原始demo时，发现之前demo中nms效果不好的现象消失了，而且更改nms的iou阈值也有效果了。然后又跑回红外的的图片发现可以去除多余的框了，但是剩余的结果并不好，所以之前并不是nms没做好，还是网络有问题。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1]: <a href="https://blog.csdn.net/koala_tree/article/details/78690396" target="_blank" rel="noopener">吴恩达cnn编程作业</a><br>[2]: <a href="https://www.jianshu.com/p/3e77cefeb49b" target="_blank" rel="noopener">python使用yolo</a><br>[3]: <a href="https://www.zhihu.com/people/chih-cheung/posts" target="_blank" rel="noopener">从零开始码一个皮卡丘检测器</a></p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-03-20-网易云吴恩达卷积神经网络</title>
    <url>/2018/03/20/2018-03-20-%E7%BD%91%E6%98%93%E4%BA%91%E5%90%B4%E6%81%A9%E8%BE%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p><img src="/2018/03/20/2018-03-20-网易云吴恩达卷积神经网络/a.jpg" alt="图片"><br><a id="more"></a></p>
<h4 id="吴恩达卷积神经网络课程"><a href="#吴恩达卷积神经网络课程" class="headerlink" title="吴恩达卷积神经网络课程"></a>吴恩达卷积神经网络课程</h4><p>课程主要是基础概念的普及.网易云课堂课程地址为<a href="https://mooc.study.163.com/learn/2001281004?tid=2001392030#/learn/content" target="_blank" rel="noopener">卷积神经网络</a>.该课程分为4部分，主要看了第一，三部分。</p>
<ul>
<li>卷积神经网络</li>
<li>深度卷积网络实例</li>
<li>目标检测</li>
<li>人脸识别和风格转换等应用</li>
</ul>
<h4 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h4><p>该部分主要有介绍了边缘检测和卷积的基本概念，以及简单示例。<br>一个关于卷积网络的细节，输入三通道图片，卷积核使用也是三通道，但是有时候书写只用3 * 3表示</p>
<p><img src="/2018/03/20/2018-03-20-网易云吴恩达卷积神经网络/1521544810294.jpg" alt="3通道卷积"></p>
<h4 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h4><p>目标检测部分讲解了yolo的一些部件，IOU，nms，anchor box。IOU（预测box和实际box的交集 / 并集），IOU越大，边界框预测越准确，IOU 一般没有小于0.5的。nms（非极大值抑制），就是抑制不是极大值的。<a href="http://blog.csdn.net/pandav5/article/details/50997272" target="_blank" rel="noopener">参考</a><br>整个流程：首先计算出所有窗口的面积，对所有窗口的分数进行从小到大排序取出最高分数的序号。循环计算1到次高分数窗口与最高分数窗口的交叉面积与两者间最小面积的比例，若小于thresh,那么把这一窗口保留之后进入下一轮。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Fast R-CNN</span></span><br><span class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></span><br><span class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></span><br><span class="line"><span class="comment"># Written by Ross Girshick</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">py_cpu_nms</span><span class="params">(dets, thresh)</span>:</span></span><br><span class="line">    <span class="string">"""Pure Python NMS baseline."""</span></span><br><span class="line">    x1 = dets[:, <span class="number">0</span>]</span><br><span class="line">    y1 = dets[:, <span class="number">1</span>]</span><br><span class="line">    x2 = dets[:, <span class="number">2</span>]</span><br><span class="line">    y2 = dets[:, <span class="number">3</span>]</span><br><span class="line">    scores = dets[:, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">    areas = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</span><br><span class="line">    order = scores.argsort()[::<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    keep = []</span><br><span class="line">    <span class="keyword">while</span> order.size &gt; <span class="number">0</span>:</span><br><span class="line">        i = order[<span class="number">0</span>]</span><br><span class="line">        keep.append(i)</span><br><span class="line">        xx1 = np.maximum(x1[i], x1[order[<span class="number">1</span>:]])</span><br><span class="line">        yy1 = np.maximum(y1[i], y1[order[<span class="number">1</span>:]])</span><br><span class="line">        xx2 = np.minimum(x2[i], x2[order[<span class="number">1</span>:]])</span><br><span class="line">        yy2 = np.minimum(y2[i], y2[order[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">        w = np.maximum(<span class="number">0.0</span>, xx2 - xx1 + <span class="number">1</span>)</span><br><span class="line">        h = np.maximum(<span class="number">0.0</span>, yy2 - yy1 + <span class="number">1</span>)</span><br><span class="line">        inter = w * h</span><br><span class="line">        ovr = inter / (areas[i] + areas[order[<span class="number">1</span>:]] - inter)</span><br><span class="line"></span><br><span class="line">        inds = np.where(ovr &lt;= thresh)[<span class="number">0</span>]</span><br><span class="line">        order = order[inds + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> keep</span><br></pre></td></tr></table></figure></p>
<h4 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h4><ul>
<li>在yolo训练时好像没有用到nms？</li>
<li>在yolo使用nms时有两个阈值，置信度阈值和nms的iou阈值</li>
<li><img src="/2018/03/20/2018-03-20-网易云吴恩达卷积神经网络/1521552821351.jpg" alt="enter description here"></li>
</ul>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
        <tag>MLDL</tag>
      </tags>
  </entry>
  <entry>
    <title>总结及其展望</title>
    <url>/2018/03/09/%E5%A4%A7%E5%9B%9B%E6%80%BB%E7%BB%93%E5%8F%8A%E5%85%B6%E5%B1%95%E6%9C%9B/</url>
    <content><![CDATA[<p><img src="/2018/03/09/大四总结及其展望/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="因缘交错的开始与结束"><a href="#因缘交错的开始与结束" class="headerlink" title="因缘交错的开始与结束"></a>因缘交错的开始与结束</h3><p>机缘巧合，高考志愿中在某两个学校中间加上了中大。又是机缘巧合来到了曾移动信息工程学院（实际上吧，是第一个志愿，只是不知道这竟然是调剂大院）。开学来了之后看到学院培养计划，发现正对胃口。然后就毫无悬念的进入了迷茫的开始。……等到现在再去回首看的时候，感觉自己四年做了好少东西，有点后悔当初怎么就不能静下心来学习。但是我知道，如果再给我一次机会，我还是会像曾经这样度过。</p>
<h3 id="一无所知的大一"><a href="#一无所知的大一" class="headerlink" title="一无所知的大一"></a>一无所知的大一</h3><p>刚进学校时有些许搞笑，一个竟然只会电脑开机的人报了计算机的方向。大一一整年，别人学习怎么做算法，而我是学着怎么用电脑。还好室友都愿意帮我，教我怎么用浏览器，怎么下载东西，怎么安装应用还告诉我尽量要自定义安装，不要全部安装在c盘……等等这一系列的别人认为理所应当每个人肯定会的东西。大一上课也遇到一些特别好的TA，CQL啊，CMD啊挺多的，但是也有很多会告诉你，你google一下就有了。哎……。印象最深刻的就是一个循环输入很多样例的算法题，因为自己也描述不清楚问题，所以就有的TA说while啊。我说我知道，但是不知道样例个数怎么停呢，他说ctrl-c。对于这种问一句答一句的TA，我只想说，我太傻。当时问了好几个TA好几天都还是不懂，差点绝望，直到死皮赖脸的问到CMD，虽然他也没懂我在描述啥，但是他告诉我这种题目怎么做，从头到尾，当他说到while(cin&gt;&gt;)时，我才知道这样啊。CQL也是特别热心的TA。不是遇到这两个人，估计我要辍学了。真的顶不住这种压力。<br>因为某些原因，本来整个人就处于特别低谷的状态。所以已经不是每个月有那么几天了，每周都有那么几天，心情像***。感觉整个人都是废的。就这样浑浑噩噩的过完了大一。来到了暑假。暑假提前一个月去了学校，没有人，自己没明没夜看来20天海贼王，七八百集看了一遍。然后突然感觉我好像忘了最初自己的样子。写了个座右铭：好想回到过去，问问小时候的自己，现在的我，你可满意。假期最后一个阶段，开始学习，当时迷上了hacker，在红盟论坛当时学到了好多东西，知道了ubuntu系统，装上了虚拟机，看到有个东西叫python。原来还有这么多好玩的东西。</p>
<h3 id="准备开始学习的大二"><a href="#准备开始学习的大二" class="headerlink" title="准备开始学习的大二"></a>准备开始学习的大二</h3><p>大二一开学就和一个室友加入了一个CL老师的实验室。进入实验室老师让我们了解DL(深度学习)，ML(机器学习的东西)。感觉挺好玩的东西，当时去听一听大三和大四师兄的分享。感觉他们好厉害。当时一开始接触就去学习一个叫caffe的框架，哈哈，除了官方文档一篇博客都没有。当时为了配CUDA，重装了无数次电脑，幸亏暑假学会了装电脑，也接触一点Ubuntu（现在cuda好配多了）。就这样在上课和接触DL相关知识中度过。<br>然后到了紧张的大二下，课程塞的满满的，除了上课就是在上课的路上。实验室根本没时间去，只有周末偶尔去，然后慢慢的周末要做作业，也没时间。渐渐的就没去了。<br>一个紧张但是碌碌无为的大二就过去了，进入暑假，来到建筑工地打工，几个字简单描述，40多度高温，住铁皮房，没空调，早上四点起，晚上睡不着。其实吧，我只是来这里找一下虐，并没有干活。</p>
<h3 id="大三分水岭"><a href="#大三分水岭" class="headerlink" title="大三分水岭"></a>大三分水岭</h3><p>经过一个暑假的找虐，和我弟聊天说想赶紧毕业出去挣钱，我弟说他也是。然后就又开始了大三上的生活，为了大三下就能去实习，所以把课选完了，这真是一个错误的决定，累死了。然后同时也发现之前老师的深度学习的那个组好像没人说话，他们应该也转移到其他小组了，所以也就没好意思再去找老师。就这样天天上课勉强度过了大三上。在寒假，和我弟聊天，大家有一致决定想好好学习，于是决定读个研。但是当时绩点有点低，和一个同学又去了HK老师这里，说了自己的想法，然后就决定参加一些比赛，发一些论文来争取一些加分。由于大三上愚蠢的决定，导致大三下没课，所以整个大三下在准备论文和比赛中度过。有点轻松，这个过程中由于一直关注着AI方向的发展，见证了caffe从没有一份博客到到处都是文档（有点后悔没坚持，不然说不定就是大佬了）。然后在这半年中系统学了一下python。然后大三上跟着CL博士师兄发了两篇文章。接着就到7月份一直在准备无人船比赛，有点难受，由于船比较大，调试船要跑到一个漏天水库，广州的夏天室外温度有点高，小组几个人每天都是轻微中暑。到7月底去上海参加了比赛，哎……效果也就那样吧，参与第一，开心就好。比赛完回来8月中旬了快，感觉自己保研无望了，就开始了复习准备考一下，赶紧进入了紧张的复习，高数线代英语先复习起来，复习完半个月，赶紧又买了计算机四大金刚相关书籍。9月份这段时间开始准备保研材料，还是不知羞耻的提交了。但是由于参加的无人船时间是7月份，保研要求只计算6月份之前的，有点担忧。然后过不久出保研结果，综绩4.0勉强过了。(虽然无人船比赛还是没有能加上分，能加0.3的，有点想哭，但是不重要了)然后面试一切结束之后，感觉就像自己准备半个月考研，然后得知考上了，心情有点复杂。总之接下来就大四了。</p>
<h3 id="又陷入迷茫的现在？"><a href="#又陷入迷茫的现在？" class="headerlink" title="又陷入迷茫的现在？"></a>又陷入迷茫的现在？</h3><p>大四上开始正式进入实验室做事，因为大一感觉到TA对人的影响，所以趁着自己有时间同时有机会，所以申请了嵌入式课程的TA。很开心的去上了几周课。然后10月份因为得知11月底在苏州常熟要举办无人车未来挑战赛，由于主办方需要我们实验室的一些帮助，所以临时被发配。在常熟的这段时间，过的很开心，因为这里有一群性格特别合适的人。在比赛的三天中由于作为裁判虽然忙的吃不上饭，但是还是很开心，亲眼目睹了跑的特别好的无人车，也目睹了好几起撞车事故。经过慎重慎重给每辆车打分的过程还是很紧张的。比赛结束后，回到了学校又继续其他项目的参与。然后就是到现在了。<br>现在每天都是在学习新的东西，每天都是在这篇文章看不懂，这份代码不会改中度过，有点紧张有点慌，但是没办法，还是要看……没啥说的，以后在回忆。</p>
<h3 id="理想？未来？"><a href="#理想？未来？" class="headerlink" title="理想？未来？"></a>理想？未来？</h3><p>命是弱者的接口运是强者的谦辞。说实话，不是很懂命运是啥，但是我知道一句话，谋事在人成事在天。至于将来到底是继续向上读，还是趁早工作，也是是很敢说。但是我知道做好现在的自己，将来已来时，做出心中的选择就行了。</p>
]]></content>
      <categories>
        <category>感想</category>
      </categories>
      <tags>
        <tag>感想</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>2018-2-1 YOLO 网络</title>
    <url>/2018/02/01/2018-2-1-YOLO-%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p><img src="/2018/02/01/2018-2-1-YOLO-网络/a.jpg" alt="图片"></p>
<a id="more"></a>
<h3 id="YOLO-网络"><a href="#YOLO-网络" class="headerlink" title="YOLO 网络"></a>YOLO 网络</h3><p>yolo网络学习的参考博客</p>
<ul>
<li><a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">YOLO_v1论文地址</a></li>
<li><a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLO_v2论文地址</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/24916786?refer=xiaoleimlnote" target="_blank" rel="noopener">图解YOLO</a></li>
<li><a href="http://blog.csdn.net/zijin0802034/article/details/53183186" target="_blank" rel="noopener">YOLO论文解读笔记1</a></li>
<li><a href="http://blog.csdn.net/tangwei2014/article/details/50915317" target="_blank" rel="noopener">YOLO论文解读笔记2</a></li>
<li><a href="http://blog.csdn.net/qq_34784753/article/details/78803423" target="_blank" rel="noopener">YOLO论文源码解析</a><h3 id="tensorflow-实现YOLO"><a href="#tensorflow-实现YOLO" class="headerlink" title="tensorflow 实现YOLO"></a>tensorflow 实现YOLO</h3>下载使用的版本为 <a href="https://github.com/hizhangp/yolo_tensorflow" target="_blank" rel="noopener">yolo_tensorflow</a>。<br>github上还有另外两版 <a href="https://github.com/nilboy/tensorflow-yolo" target="_blank" rel="noopener">tensorflow_yolo</a> 和 <a href="https://github.com/ruiminshen/yolo-tf" target="_blank" rel="noopener">yolo-tf</a>。<br>此外还有一款基于Pytorch实现的yolo_v2 <a href="https://github.com/ruiminshen/yolo2-pytorch" target="_blank" rel="noopener">yolo2-pytorch</a>，pytorch 版本的项目目前在持续维护中，被推荐使用该版本。</li>
</ul>
<h3 id="YOLO-学习"><a href="#YOLO-学习" class="headerlink" title="YOLO 学习"></a>YOLO 学习</h3><p><a href="http://noahsnail.com/2017/08/02/2017-8-2-YOLO%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/" target="_blank" rel="noopener">YOLO论文中文翻译</a>翻译质量有点差。<br><a href="http://blog.csdn.net/weixin_35654926/article/details/72473024" target="_blank" rel="noopener">YOLO_v2中文翻译</a><br>文章目前还没看懂。</p>
<h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p>使用别人训练的模型，并没有什么效果。</p>
<p> <img src="/2018/02/01/2018-2-1-YOLO-网络/1521553976129.jpg" alt="行人"> </p>
<p> <img src="/2018/02/01/2018-2-1-YOLO-网络/1521554025867.jpg" alt="car"></p>
<h3 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h3><ol>
<li>使用tensorflow时出现’module object has no attribute ‘contrib’’问题。解决方式是更新tensorflow 版本到1.0以上<code>pip install --upgrade tensorflow-gpu</code>或者<code>pip install --upgrade tensorflow</code>。<a href="http://blog.csdn.net/xyj1536214199/article/details/70766580" target="_blank" rel="noopener">参考</a></li>
<li><p>python3导入cv2时和ros冲突。<a href="http://blog.csdn.net/yjy728/article/details/71403271" target="_blank" rel="noopener">解决方式</a>是暂时屏蔽ros里的opencv，需要用ros的话还得<a href="http://stackoverflow.com/questions/43019951/after-install-ros-kinetic-cannot-import-opencv" target="_blank" rel="noopener">改回来</a>.</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gedit /home/abner/.bashrc</span><br></pre></td></tr></table></figure>
<p> 注释掉最后几行中的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">source /opt/ros/kinetic/setup.bash</span><br></pre></td></tr></table></figure>
<p>保存之后source 然后重启终端</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>note</tag>
        <tag>MLDL</tag>
      </tags>
  </entry>
</search>
